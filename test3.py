'''
from PIL import Image
import numpy as np
import torch
import torchstain
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import os
from tqdm import tqdm

class TorchstainWithStandardRef:
    def __init__(self, backend='torch'):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.normalizer = torchstain.normalizers.MacenkoNormalizer(backend=backend)
        self._set_standard_reference()

    def _set_standard_reference(self):
        HERef = np.array([[0.5626, 0.2159],
                          [0.7201, 0.8012],
                          [0.4062, 0.5581]])
        maxCRef = np.array([1.9705, 1.0308])

        self.normalizer.stain_matrix = torch.tensor(HERef, dtype=torch.float32).to(self.device)
        self.normalizer.maxC = torch.tensor(maxCRef, dtype=torch.float32).to(self.device)

    def apply_tensor(self, img_tensor):
        norm, _, _ = self.normalizer.normalize(I=img_tensor.to(self.device), stains=True)
        norm_img = norm.byte().cpu().numpy()
        return Image.fromarray(norm_img)


class PatchDataset(Dataset):
    def __init__(self, image_dir):
        self.image_paths = [os.path.join(image_dir, fname)
                            for fname in os.listdir(image_dir)
                            if fname.lower().endswith(('png', 'jpg', 'jpeg', 'tif', 'tiff'))]
        self.T = transforms.Compose([
            transforms.Resize((224, 224)),  # ğŸ‘ˆ ä¿è¯æ‰€æœ‰å›¾åƒå¤§å°ä¸€æ ·,ä»…æµ‹è¯•æ—¶ä½¿ç”¨
            transforms.ToTensor(),
            transforms.Lambda(lambda x: x * 255)
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        path = self.image_paths[idx]
        img = Image.open(path).convert("RGB")
        return self.T(img), os.path.basename(path)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--input_dir', type=str, required=True, help='è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹')
    parser.add_argument('--output_dir', type=str, required=True, help='ä¿å­˜å½’ä¸€åŒ–å›¾åƒçš„æ–‡ä»¶å¤¹')
    parser.add_argument('--batch_size', type=int, default=32, help='GPUæ‰¹å¤„ç†å¤§å°')
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    dataset = PatchDataset(args.input_dir)
    loader = DataLoader(dataset, batch_size=args.batch_size, num_workers=4, pin_memory=True)

    normalizer = TorchstainWithStandardRef()

    for batch_imgs, batch_names in tqdm(loader, desc="å½’ä¸€åŒ–å¤„ç†ä¸­"):
        for i in range(len(batch_imgs)):
            try:
                norm_img = normalizer.apply_tensor(batch_imgs[i])
                norm_img.save(os.path.join(args.output_dir, batch_names[i]))
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {batch_names[i]}, é”™è¯¯: {e}")

    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
'''
import os
import argparse
import csv

def count_pngs(root_dir):
    """
    éå† root_dir ä¸‹æ‰€æœ‰å­ç›®å½•ï¼ˆé€’å½’ï¼‰ï¼Œ
    æ”¶é›†æ¯ä¸ªç›®å½•ä¸­ .png æ–‡ä»¶çš„æ•°é‡ï¼ˆåªè®°å½• >0 çš„ç›®å½•ï¼‰ã€‚
    è¿”å›åˆ—è¡¨ [(dirpath, png_count), ...]ã€‚
    """
    results = []
    for dirpath, _, filenames in os.walk(root_dir):
        png_count = sum(1 for fname in filenames if fname.lower().endswith('.png'))
        if png_count > 0:
            results.append((dirpath, png_count))
    return results

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ç»Ÿè®¡ç›®å½•ä¸‹å„å­ç›®å½•çš„ PNG æ•°é‡ï¼Œå¹¶è¾“å‡º CSV')
    parser.add_argument('root_dir', help='è¦æ£€æŸ¥çš„æ ¹ç›®å½•')
    parser.add_argument('--output_csv', default='png_counts.csv', help='è¾“å‡ºçš„ CSV æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    data = count_pngs(args.root_dir)
    # å†™å…¥ CSV
    with open(args.output_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['directory', 'png_count'])
        writer.writerows(data)

    print(f"âœ… å·²å°†ç»“æœä¿å­˜åˆ° CSV: {args.output_csv}")